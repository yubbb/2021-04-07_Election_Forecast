{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup 요청 함수\n",
    "def getSource(site) :\n",
    "    \n",
    "    import requests\n",
    "    import bs4\n",
    "    \n",
    "    # 헤더 정보\n",
    "    header_info = {\n",
    "        'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWeb Kit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.146 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    # 요청한다.\n",
    "    response = requests.get(site, headers=header_info)\n",
    "    \n",
    "    # bs4 객체 생성\n",
    "    soup = bs4.BeautifulSoup(response.text, 'lxml')\n",
    "    \n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한 페이지에 있는 다음 뉴스 링크 수집 함수\n",
    "def getNewsLink(site, COLOPHON):\n",
    "    \n",
    "    import pandas as pd\n",
    "    import os\n",
    "    \n",
    "    soup = getSource(site)\n",
    "\n",
    "    link_list = []\n",
    "\n",
    "    # li 태그 가져오기\n",
    "    a1 = soup.select('#clusterResultUL > li')\n",
    "    # print(len(a1))\n",
    "    \n",
    "    for a2 in a1:\n",
    "        \n",
    "        # div 태그 가져오기\n",
    "        a3 = a2.select_one('div.wrap_cont > div > div > a')\n",
    "        # print(a3)\n",
    "        \n",
    "        # 기사링크 \n",
    "        data1 = a3.attrs['href']\n",
    "        # print(data1)\n",
    "        \n",
    "        # 기사제목\n",
    "        data2 = a3.text.strip()\n",
    "        # print(data2)\n",
    "    \n",
    "        # span 태그 가져오기\n",
    "        a4 = a2.select_one('div.wrap_cont > div > span.f_nb.date')\n",
    "        a5 = a4.text.strip().split('|')\n",
    "        \n",
    "        # 날짜\n",
    "        data3 = a5[0]\n",
    "        \n",
    "        # 언론사\n",
    "        data4 = a5[1]\n",
    "        \n",
    "        # print(data1, data2, data3, data4)\n",
    "        \n",
    "        # 기사 링크 리스트에 저장\n",
    "        link_list.append(data1)\n",
    "    \n",
    "    # 데이터프레임 생성\n",
    "    df1 = pd.DataFrame(link_list)\n",
    "    # display(df1)\n",
    "    \n",
    "\n",
    "    FILENAME = f'{COLOPHON}_link.csv'\n",
    "\n",
    "    if os.path.exists(FILENAME) == False:\n",
    "        # 파일이 없을 경우\n",
    "        df1.to_csv(FILENAME, encoding='utf-8-sig', index=False)\n",
    "    else:\n",
    "        # mode='a' : 기존 것 뒤에다 붙여줌\n",
    "        df1.to_csv(FILENAME, encoding='utf-8-sig', index=False, header=False, mode='a')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다음 페이지 존재 여부 확인하는 함수\n",
    "def getNextPage(site) :\n",
    "    \n",
    "    # url에서 p= 값 들고오기\n",
    "    p = site.split('&')[-1].split('=')[-1]\n",
    "    \n",
    "    # p값에 1 더해서 다음 페이지 url 만들기\n",
    "    nextPage = site[:-len(p)] + str(int(p)+1)\n",
    "    # print(next_page)\n",
    "\n",
    "    # 현재 페이지와 다음 페이지 soup 가져오기\n",
    "    soup1 = getSource(site)\n",
    "    soup2 = getSource(nextPage)\n",
    "    # print(soup1)\n",
    "    # print(soup2)\n",
    "\n",
    "    # 현재 페이지와 다음 페이지 첫번째 a 태그에서 링크 가져오기\n",
    "    a1 = soup1.select('#clusterResultUL > li > div.wrap_cont > div > div > a')[0].attrs['href']\n",
    "    a2 = soup2.select('#clusterResultUL > li > div.wrap_cont > div > div > a')[0].attrs['href']\n",
    "    # print(a1)\n",
    "    # print(a2)\n",
    "   \n",
    "    # 두 링크가 같지 않으면 다음 페이지가 있다고 간주, 다음 페이지 return \n",
    "    if a1 != a2 :\n",
    "        return True\n",
    "    # 같으면 다음 페이지 없다고 간주, False return \n",
    "    else :\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDaumNewsUrlDF(KEYWORD, COLOPHON, dayStart, dayEnd, page):\n",
    "\n",
    "    import time\n",
    "    import pandas as pd\n",
    "    from IPython.display import clear_output\n",
    "    import urllib\n",
    "    \n",
    "    # 검색어\n",
    "    KEYWORD = urllib.parse.quote(KEYWORD)\n",
    "    COLOPHON = COLOPHON\n",
    "    dayStart = dayStart\n",
    "    dayEnd = dayEnd\n",
    "    page=page\n",
    "    \n",
    "    # 다음 뉴스 검색 url\n",
    "    URL = 'https://search.daum.net/search?w=news&enc=utf8&cluster=y&cluster_page=1&'\n",
    "    \n",
    "    # url에 들어갈 파라미터\n",
    "    cp_dict = {'조선일보' : '16d4PV266g2j-N3GYq',\n",
    "               '중앙일보' : '16Elf9uX5H6T5xXvQV',\n",
    "               '동아일보' : '16bOiOx4gG2S18EPLj',\n",
    "               'JTBC'     : '16yZfDfR_rGcw5F-P0',\n",
    "               '경향신문' : '16akMkKFDu6n8GTzZr',\n",
    "               '한겨레' : '16nzyJHdH5ORpabfqG'}\n",
    "    cpName = urllib.parse.quote(COLOPHON)\n",
    "    cp = cp_dict[COLOPHON]\n",
    "    \n",
    "    # 페이지 번호\n",
    "    page = 1\n",
    "    \n",
    "    while True :\n",
    "        time.sleep(1)\n",
    "\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        site = f'{URL}q={KEYWORD}&cpname={cpName}&cp={cp}&period=6m&sd={dayStart}&ed={dayEnd}&DA=PGD&p={page}'\n",
    "\n",
    "        print(f' 다음 뉴스 - {COLOPHON} : {page} 페이지 수집 중' )\n",
    "\n",
    "        getNewsLink(site, COLOPHON) \n",
    "        chk = getNextPage(site)\n",
    "\n",
    "        if chk != False:\n",
    "            page = page + 1\n",
    "        else: \n",
    "            print(f'{COLOPHON}_link.csv 파일 저장 완료')\n",
    "            break\n",
    "    \n",
    "    df = pd.read_csv(f'{COLOPHON}_link.csv', )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 다음 뉴스 - 동아일보 : 5 페이지 수집 중\n",
      "동아일보_link.csv 파일 저장 완료\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>링크</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://v.media.daum.net/v/20210407160923444?f=o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://v.media.daum.net/v/20210407160849414?f=o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://v.media.daum.net/v/20210407191759929?f=o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://v.media.daum.net/v/20210407172950294?f=o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://v.media.daum.net/v/20210407164443707?f=o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>http://v.media.daum.net/v/20210407030146242?f=o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>http://v.media.daum.net/v/20210407030145241?f=o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>http://v.media.daum.net/v/20210407030402404?f=o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>http://v.media.daum.net/v/20210407030326361?f=o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>http://v.media.daum.net/v/20210407030159263?f=o</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>245 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  링크\n",
       "0    http://v.media.daum.net/v/20210407160923444?f=o\n",
       "1    http://v.media.daum.net/v/20210407160849414?f=o\n",
       "2    http://v.media.daum.net/v/20210407191759929?f=o\n",
       "3    http://v.media.daum.net/v/20210407172950294?f=o\n",
       "4    http://v.media.daum.net/v/20210407164443707?f=o\n",
       "..                                               ...\n",
       "240  http://v.media.daum.net/v/20210407030146242?f=o\n",
       "241  http://v.media.daum.net/v/20210407030145241?f=o\n",
       "242  http://v.media.daum.net/v/20210407030402404?f=o\n",
       "243  http://v.media.daum.net/v/20210407030326361?f=o\n",
       "244  http://v.media.daum.net/v/20210407030159263?f=o\n",
       "\n",
       "[245 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = False\n",
    "if test:\n",
    "    # 검색어\n",
    "    KEYWORD = '보궐선거'\n",
    "\n",
    "    # 언론\n",
    "    # 조선일보, 중앙일보, 동아일보, JTBC, 경향신문, 한겨레 택1\n",
    "    COLOPHON = '동아일보'\n",
    "\n",
    "    # 날짜 (YYYYMMDhhmmss)\n",
    "    dayStart = '20210407000000'\n",
    "    dayEnd   = '20210407200000'\n",
    "\n",
    "    df = getDaumNewsUrlDF(KEYWORD=KEYWORD, COLOPHON=COLOPHON, dayStart=dayStart, dayEnd=dayEnd, page=1)\n",
    "    display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
